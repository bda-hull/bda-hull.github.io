<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

	<title>Big Data Analytics Research Group</title>

	<!-- Google font -->
	<link href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CVarela+Round" rel="stylesheet">

	<!-- Bootstrap -->
	<link type="text/css" rel="stylesheet" href="css/bootstrap.min.css" />

	<!-- Owl Carousel -->
	<link type="text/css" rel="stylesheet" href="css/owl.carousel.css" />
	<link type="text/css" rel="stylesheet" href="css/owl.theme.default.css" />

	<!-- Magnific Popup -->
	<link type="text/css" rel="stylesheet" href="css/magnific-popup.css" />

	<!-- Font Awesome Icon -->
	<link rel="stylesheet" href="css/font-awesome.min.css">

	<!-- Custom stlylesheet -->
	<link type="text/css" rel="stylesheet" href="css/style.css" />

	<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
		<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
		<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
</head>

<body>

	<!-- Header -->
	<header>

		<!-- Nav -->
		<nav id="nav" class="navbar">
			<div class="container">

				<div class="navbar-header">
					<!-- Logo -->
					<div class="navbar-brand">
						<a href="index.html">
							<img class="logo" src="img/logo.png" alt="logo">
						</a>
					</div>
					<!-- /Logo -->

					<!-- Collapse nav button -->
					<div class="nav-collapse">
						<span></span>
					</div>
					<!-- /Collapse nav button -->
				</div>

			<!--  Main navigation  -->
				<ul class="main-nav nav navbar-nav navbar-right">
					<li><a href="index.html#home">Home</a></li>
					<li><a href="index.html#research">Research</a>
					<li><a href="publications.html">Publications</a></li>
					<li><a href="index.html#team">Team</a></li>
					<li><a href="index.html#news">News, Projects + Activities</a></li>
					<li><a href="index.html#contact">Contact</a></li>
				</ul>
				<!-- /Main navigation -->

			</div>
		</nav>
		<!-- /Nav -->

		<!-- header wrapper -->
		<div class="header-wrapper sm-padding bg-grey">
			<div class="container">
				<h2>DEEP LEARNING</h2>
				<ul class="breadcrumb">
					<li class="breadcrumb-item"><a href="index.html">Home</a></li>
					<li class="breadcrumb-item"><a href="index.html#research">Research</a></li>
				</ul>
			</div>
		</div>
		<!-- /header wrapper -->

	</header>
	<!-- /Header -->

	<!-- Blog -->
	<div id="blog" class="section md-padding">

		<!-- Container -->
		<div class="container">
			<!-- Row -->
			<div class="row">

				<!-- Main -->
				<main id="main" class="col-md-9">
					<div class="blog">
						<div class="blog-img">
<!--					<img class="img-responsive" src="./img/rnnc.jpg" alt="" width="800">-->							
						</div>
						<div class="blog-content">

							<h3><br/> Our Deep Learning Research</h3>
							
							<p>We apply machine learning in all our research projects, most recently deep learning in particular due to its suitability for feature learning from large datasets and its ability to identify patterns in large and noisy datasets. </p>								
														<img class="img-responsive" src="./img/rnnc.jpg" alt="" width="350" align="right">																					
							<p>Most of our work on natural language processing and other time-series applications uses LSTM/GRU models (e.g. for <a href="publications/wisdom2018schoene.pdf">sentiment analysis</a>, <a href="publications/ieee_ci_final_draft.pdf">natural language generation</a>) while other projects in renewable energy and healthcare have used mainly CNNs.</p>							
							<p>Current projects include a PhD scholarship by <a href="http://greenporthull.co.uk/">Greenport Growth</a>  for deep learning for offshore wind energy in collaboration with <a href="https://aurawindenergy.com/">Project Aura</a> and 5 University PhD scholarships for work on or related to deep learning. </p><p>We run and host the annual Deep Learning Winter School (funded by the Ferens Education Trust) and offer a range of student projects in this area too. </br></br></p>										
							
<h5>Language Learning and Domain Transfer</h5>							
							<p>Research beyond applications of deep learning to specific domains looks mainly into domain transfer of deep learning models. For example, we have developed a technique that allows <a href="nlp.html">natural language generation systems</a> to produce outputs for completely new and unseen domains by drawing on a set of common abstract linguistic inputs identified from other domains and achieving up to <a href="publications/ieee_ci_final_draft.pdf">100% of source domain performance</a>. This is useful as it reduces the need for large amounts of clean and high-quality data for each new domain, thereby reducing development time, costs and human effort and making a significant step towards automatic induction of language generation systems. </br></br></p>	
							
<h5>Fast Protoyping</h5>							
										
							<img class="img-responsive" src="./img/pca.jpg" alt="" width="250" align="right">														
							<p>We have also investigated shortcuts in developing deep learning models for new domains per se, including domain specific programming languages for fast prototyping based on an underspecific input representation that is automatically optimised for a new learning problem. Other research includes domain transfer for hyperparameter optimisation. Here, we found that development time for deep learning models can be reduced by up to 70% by reusing parameters that are known to be optimal in a domain that is similar to a current target domain across a number of objective characteristics.</p>																	
						</div>

																	
	
	<!-- Portfolio -->
	<div id="projects" class="section md-padding">

		<!-- Container -->
		<div class="container">

			<!-- Row -->
			<div class="row">						
					
				</main>
				<!-- /Main -->


				<!-- Aside -->
				<aside id="aside" class="col-md-3">

					<!-- Category -->
					<div class="widget">
						<h3 class="title">Categories</h3>
						<div class="widget-category">
							<a href="deep_learning.html#main">Deep Learning</a>
							<a href="evolutionary_algorithms.html#main">Evolutionary Algorithms</a>
							<a href="interactive_systems.html#main">Interactive Systems</a>
							<a href="nlp.html#main">Natural Language Processing</a>
							<a href="bio_health.html#main">Biology and Health</a>
							<a href="natural.html#main">Data and Natural World</a>							
						</div>
					</div>
					<!-- /Category -->

					<!-- Posts sidebar -->
					<div class="widget">
						<h3 class="title">News Stuff</h3>
						
						<!-- single post -->
						<div class="widget-post">
							<a href="https://nips.cc/">
								<img src="./img/neurips2019-post.jpeg" alt="">NeurIPS in Vancouver</a>
							<ul class="blog-meta">
								<li>Dec 2019</li>
							</ul>
						</div>
						<!-- /single post -->

						<!-- single post -->
						<div class="widget-post">
							<a href="https://thespencergroup.co.uk/">
								<img src="./img/rnnc2-post.jpg" alt="">KTP project with Spencer Group</a>
							<ul class="blog-meta">
								<li>Oct 2019</li>
							</ul>
						</div>
						<!-- /single post -->

						<!-- single post -->
						<div class="widget-post">
							<a href="https://www.linkedin.com/feed/update/urn%3Ali%3Aactivity%3A6596404880460611584/?midToken=AQHBPjnIWTOZbg&trk=eml-email_notification_single_mentioned_you_in_this_01-notifications-1-hero%7Ecard%7Efeed&trkEmail=eml-email_notification_single_mentioned_you_in_this_01-notifications-1-hero%7Ecard%7Efeed-null-522tij%7Ek2ho9beq%7Er5-null-voyagerOffline">
								<img src="./img/hackathon-glasgow-post.jpeg" alt=""> Offshore Wind Hackathon, Glasgow</a>
							<ul class="blog-meta">
								<li>Oct 2019</li>
							</ul>
						</div>
						<!-- /single post -->

						<!-- single post -->
						<div class="widget-post">
							<a href="http://auracdt.hull.ac.uk/">
								<img src="./img/cdt-post.jpg" alt=""> Aura Centre for Doctoral Training in Offshore Wind
							and the Environment</a>
							<ul class="blog-meta">
								<li>4 Feb 2019</li>
							</ul>
						</div>
						<!-- /single post -->

						<!-- single post -->
						<div class="widget-post">
							<a href="https://www.eventbrite.com/e/gpu-programming-for-deep-learning-tickets-53947006950">
								<img src="./img/rnnc2-post.jpg" alt=""> GPU programming for deep learning - 24/25 Jan.
							</a>
							<ul class="blog-meta">
								<li>20 Dec 2018</li>
							</ul>
						</div>
						<!-- /single post -->

					<!-- single post -->
						<div class="widget-post">
							<a href="news-xmas-2018.html">
								<img src="./img/xmas-2018-post.jpg" alt=""> Christmas Poster Workshop 2018
							</a>
							<ul class="blog-meta">
								<li>7 Dec 2018</li>
							</ul>
						</div>
						<!-- /single post -->	

						<!-- single post -->
						<div class="widget-post">
							<a href="news-catapult-joyjit-2018.html">
								<img src="./img/catapult-post.jpg" alt=""> OREC Catapult in Glasgow
							</a>
							<ul class="blog-meta">
								<li>22 Nov 2018</li>
							</ul>
						</div>
						<!-- /single post -->
						

						<a href="index.html#news"> --- ALL NEWS --- </a>																			

					</div>
					<!-- /Posts sidebar -->

				</aside>
				<!-- /Aside -->

			</div>
			<!-- /Row -->

		</div>
		<!-- /Container -->

	</div>
	<!-- /Blog -->
	
	
	
	
	<!-- Portfolio -->
	<div id="publications" class="section md-padding bg-grey">

		<!-- Container -->
		<div class="container">

			<!-- Row -->
			<div class="row">

				<!-- Section header -->
				<div class="section-header text-center">
					<h2 class="title">Deep Learning Publications</h2>
				</div>

			<main class="section ">
					<div class="blog">

						<!-- publication entry -->
						
						<!-- publication entry -->

						<div class="blog-content">
							<h3>Transparent Deep Learning and Transductive Transfer Learning: A New Dimension for Wind Energy Research.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="joyjit.html">Chatterjee, J.</a>
								</li>							
							<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>									
								<li>
								<!--<a href="https://www.climatechange.ai/CameraReadySubmissions%202-119/22/CameraReadySubmission/neurips_2019.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>	 -->
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										Wind turbines suffer from operational inconsistencies due to a variety of factors, ranging from environmental changes, to intrinsic anomalies in specific components, such as gearbox, generator, pitch system etc.  Condition monitoring of wind turbines has been a critical research area in the last decade, wherein the Supervisory Control & Data Acquisition (SCADA) data is used to analyse the operational behaviour of the turbine and predict any incipient faults to prevent catastrophic losses caused by unexpected failures. Machine learning models have formed a large part of the data-analytics based methods used for learning from historical failures through supervised learning, but they suffer from the lack of ability to provide additional capabilities for learning with little labelled data, or for that matter, no labelled faults in a different domain. Deep learning has shown immense success in areas where time-series data is to be modelled. In this paper, we propose a hybrid deep learning model combining a Long short-term memory network (LSTM) with XGBoost, a decision tree-based classifier for providing the benefits of accuracy through deep learning, and transparency through traditional decision trees. Our study shows that Transfer learning allows us to make predictions with increasing accuracy on unseen data; which is useful for simulations of new operations, new wind farms or other cases of non-available training data. This can help reduce downtime of turbines through predictive maintenance, by predicting incipient faults, or provide corrective maintenance, by assisting the engineers and technicians to analyse the root causes behind the failure, thus contributing to the reliability and uptake of wind energy as a sustainable and promising domain. 
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2019. In WindEurope Offshore, Copenhagen, Denmark.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
								<a href="natural.html#publications">
									<i class="fa fa-tag"></i>Data and Natural World</a>									
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->						
						
						<!-- publication entry -->

						<div class="blog-content">
							<h3>Natural Language Generation for Operations and Maintenance in Wind Turbines.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="joyjit.html">Chatterjee, J.</a>
								</li>							
							<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>									
								<li>
								<a href="https://www.climatechange.ai/CameraReadySubmissions%202-119/22/CameraReadySubmission/neurips_2019.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>	 
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										Wind energy is one of the fastest-growing sustainable energy sources in the world
but relies crucially on efficient and effective operations and maintenance to generate
sufficient amounts of energy and reduce downtime of wind turbines and associated
costs. Machine learning has been applied to fault prediction in wind turbines,
but these predictions have not been supported with suggestions on how to avert
and fix faults. We present a data-to-text generation system using transformers to
produce event descriptions from SCADA data capturing the operational status of
turbines and proposing maintenance strategies. Experiments show that our model
learns feature representations that correspond to expert judgements. In making a
contribution to the reliability of wind energy, we hope to encourage organisations
to switch to sustainable energy sources and help combat climate change.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2019. In NeurIPS 2019 Workshop on Tackling Climate Change with Machine Learning. Vancouver, Canada.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
								<a href="natural.html#publications">
									<i class="fa fa-tag"></i>Data and Natural World</a>									
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->						
						
						<!-- publication entry -->

						<div class="blog-content">
							<h3>Hybrid approaches to fine-grained emotion detection in social media data.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="annika.html">Schoene, A.</a>
								</li>							
								<li>
								<a href="publications/annika_aaai2020_dc.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>	 
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										This paper states the challenges in fine-grained target-
dependent Sentiment Analysis for social media data using recurrent neural networks. Firstly, we outline the problem statement and give a brief overview of related work in the area. Then we outline progress and results achieved to date, a brief
research plan and future directions of this work.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>To appear. In AAAI-2020 Doctoral Consortium. New York, USA.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->						

						<div class="blog-content">
							<h3>Bidirectional Dilated LSTM with Attention for Fine-grained Emotion Classification in Tweets.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="annika.html">Schoene, A.</a>
								</li>							
								<li>
									<i class="fa fa-user"></i>
									<a href="alex.html">Turner, A.</a>
								</li>									
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
								<!--	<a href="https://aiforsocialgood.github.io/neurips2019/accepted/track1/pdfs/45_aisg_neurips2019.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>	 -->
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										We propose a novel approach for fine-grained emotion classification in tweets using a Bidirectional Dilated LSTM (BiDLSTM) with attention. Conventional LSTM architectures can face problems when classifying long sequences, which is problematic for tweets, where
crucial information is often attached to the end of a sequence, e.g. an emoticon. We show that by adding a bidirectional layer, dilations and attention mechanism to a standard LSTM, our model overcomes these problems and is able to maintain complex data
dependencies over time. We present experiments with two datasets,
the 2018 WASSA Implicit Emotions Shared Task and a new dataset
of 240,000 tweets. Our BiDLSTM with attention achieves a test
accuracy of up to 81.97% outperforming competitive baselines by
up to 10.52% on both datasets. Finally, we evaluate our data against
a human benchmark on the same task.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>To appear. In Proceedings of AAAI-2020 Workshop on Affective Content Analysis. New York, USA.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Dilated LSTM with ranked units for classification of suicide notes.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="annika.html">Schoene, A.</a>
								</li>			
					<li>
									<i class="fa fa-user"></i>
									<a href="alex.html">Turner, A.</a>
								</li>								 				
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<a href="https://aiforsocialgood.github.io/neurips2019/accepted/track1/pdfs/45_aisg_neurips2019.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>	
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										Recent statistics in suicide prevention show that people are increasingly posting
their last words online and with the unprecedented availability of textual data
from social media platforms researchers have the opportunity to analyse such data.
Furthermore, psychological studies have shown that our state of mind can manifest
itself in the linguistic features we use to communicate. In this paper, we investigate
whether it is possible to automatically identify suicide notes from other types of
social media blogs in a document-level classification task. Also, we present a
learning model for modelling long sequences, achieving an f1-score of 0.84 over
the baselines of 0.53 and 0.80 (best competing model). Finally, we also show
through visualisations which features the learning model identifies.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2019. In AI for Social Good workshop at NeurIPS (2019), Vancouver, Canada.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
								<a href="bio_health.html#publications">
									<i class="fa fa-tag"></i>Biology and Health</a>									
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Dilated LSTM with attention for Classification of suicide notes.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="annika.html">Schoene, A.</a>
								</li>
					<li>
									<i class="fa fa-user"></i>
									<a href="george.html">Lacey, G.</a>
								</li>				
					<li>
									<i class="fa fa-user"></i>
									<a href="alex.html">Turner, A.</a>
								</li>								 				
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<a href="https://www.aclweb.org/anthology/D19-6217.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>	
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										In this paper we present a dilated LSTM with
attention mechanism for document-level classification of suicide notes, last statements and
depressed notes. We achieve an accuracy of
87.34% compared to competitive baselines of
80.35% (Logistic Model Tree) and 82.27%
(Bi-directional LSTM with Attention). Furthermore, we provide an analysis of both the
grammatical and thematic content of suicide
notes, last statements and depressed notes. We
find that the use of personal pronouns, cognitive processes and references to loved ones are
most important. Finally, we show through visualisations of attention weights that the Dilated LSTM with attention is able to identify
the same distinguishing features across documents as the linguistic analysis.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2019. In Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019)  at EMNLP. Hong Kong.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->
						
						<!-- publication entry -->					
					
		<div class="blog-content">
							<h3>A Deep Learning Approach Towards Prediction of Faults in Wind Turbines.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="joyjit.html">Chatterjee, J.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>					
									<a href="http://nldl2019.org/"><i class="fa fa-external-link"></i></a>Link to workshop</li>									
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										With the rising costs of conventional sources of en- ergy, the world is moving towards sustainable energy sources including wind energy. Wind turbines consist of several electrical and mechanical components and experience an enormous amount of irregular loads, making their operational behaviour at times inconsis- tent. Operations and Maintenance (O&M) is a key factor in monitoring such inconsistent behaviour of the turbines in order to predict and prevent any in- cipient faults which may occur in the near future.
										</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2019. Extended Abstract in Northern Lights Deep Learning Workshop (NLDL), Tromso, Norway.</li>
							</ul>
						<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
								<a href="natural.html#publications">
									<i class="fa fa-tag"></i>Data and Natural World</a>									
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->


						<!-- publication entry -->

						<div class="blog-content">
							<h3>Unsupervised suicide note classification.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="annika.html">Schoene, A.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
									<li>
									<a href="publications/wisdom2018schoene.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>	
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										With the greater availability of linguistic data from public social media platforms and the advancements of natural language processing, a number of opportunities have arisen for researchers to analyse this type of data. Research efforts have mostly focused on detecting the polarity of textual data, evaluating whether there is positive, negative or sometimes neutral content. Especially the use of neural networks has recently yielded significant results in polarity detection experiments. In this paper we present a more fine-grained approach to detecting sentiment in textual data, particularly analysing a corpus of suicide notes, depressive notes and love notes. We achieve a classification accuracy of 71.76% when classifying based on text and sentiment features, and an accuracy of 69.41% when using the words present in the notes alone. We discover that while emotions in all three datasets overlap, each of them has a unique ‘emotion profile’ which allows us to draw conclusions about the potential mental state that is reflects. Using the emotion sequences only, we achieve an accuracy of 75.29%. The results from unannotated data, while worse than the other models, nevertheless represent an encouraging step towards being able to flag potentially harmful social media posts online and in real time. We provide a high-level corpus analysis of the data sets in order to demonstrate the grammatical and emotional differences.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2018. In Proceedings of the 7th KDD Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM), co-located
									with the Knowledge Discovery and Data Mining (KDD), London, UK.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Domain Transfer for Deep Natural Language Generation from Abstract Meaning Representations.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<a href="publications/ieee_ci_final_draft.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										Stochastic natural language generation systems that are trained from labelled datasets are often domain-specific in their annotation and in their mapping from semantic input representations to lexical-syntactic outputs. As a result, learnt models fail to generalize across domains, heavily restricting their usability beyond single applications. In this article, we focus on the problem of domain adaptation for natural language generation. We show how linguistic knowledge from a source domain, for which labelled data is available, can be adapted to a target domain by reusing training data across domains. As a key to this, we propose to employ abstract meaning representations as a common semantic representation across domains. We model natural language generation as a long short- term memory recurrent neural network encoder-decoder, in which one recurrent neural network learns a latent representation of a semantic input, and a second recurrent neural network learns to decode it to a sequence of words. We show that the learnt representations can be transferred across domains and can be leveraged effectively to improve training on new unseen domains. Experiments in three different domains and with six datasets demonstrate that the lexical-syntactic constructions learnt in one domain can be transferred to new domains and achieve up to 75-100% of the performance of in-domain training. This is based on objective metrics such as BLEU and semantic error rate and a subjective human rating study. Training a policy from prior knowledge from a different domain is consistently better than pure in-domain training by up to 10%.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2017. IEEE Computational Intelligence Magazine: Special Issue on Natural Language Generation with Computational Intelligence.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
								<a href="interactive_systems.html#publications">
									<i class="fa fa-tag"></i>Interactive Systems</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Transparency Of Execution Using Epigenetic Networks.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="alex.html">Turner, A.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<a href="https://www.mitpressjournals.org/doi/abs/10.1162/ecal_a_0068_14"><i class="fa fa-external-link"></i></a>Link to article</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										This paper describes how the recurrent connectionist architecture epiNet, which is capable of dynamically modifying its topology, is able to provide a form of transparent execution. EpiNet, which is inspired by eukaryotic gene regulation in nature, is able to break its own architecture down into sets of smaller interacting networks. This allows for autonomous complex task decomposition, and by analysing these smaller interacting networks, it is possible to provide a real world understanding of why specific decisions have been made. We expect this work to be useful in fields where the risk of improper decision making is high, such as medical simulations, diagnostics and financial modelling. To test this hypothesis we apply epiNet to two data sets within UCI’s machine learning repository, each of which requires a specific set of behaviours to solve. We then perform analysis on the overall functionality of epiNet in order to deduce the underlying rules behind its functionality and in turn provide transparency of execution.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2017. In Proceedings of the European Conference on Artificial Life (ECAL), Lyon, France.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="evolutionary_algorithms.html#publications">
									<i class="fa fa-tag"></i>Evolutionary Algorithms</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Deep text generation - Using hierarchical decomposition to mitigate the effect of rare data points.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>
									<a href="alex.html">Turner, A.</a>
								</li>
								<li>
									<a href="publications/ldk2017.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										Deep learning has recently been adopted for the task of natural language generation (NLG) and shown remarkable results. However, learning can go awry when the input dataset is too small or not well balanced with regards to the examples it contains for various input sequences. This is relevant to naturally occurring datasets such as many that were not prepared for the task of natural language processing but scraped off the web and originally prepared for a different purpose. As a mitigation to the problem of unbalanced training data, we therefore propose to decompose a large natural language dataset into several subsets that “talk about” the same thing. We show that the decomposition helps to focus each learner’s attention during training. Results from a proof-of-concept study show 73% times faster learning over a flat model and better results.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2017. In Proceedings of Language, Data and Knowledge (LDK), Galway, Ireland. Proceedings in: Springer Lecture Notes
									in Computer Science (LNCS).</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>DEFIne: A Fluent Interface DSL for Deep Learning Applications.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>Hawick, K.</li>
								<li>
									<a href="publications/rwdsl2017-final.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										Recent years have seen a surge of interest in deep learning models that outperform other machine learning algorithms on benchmarks across many disciplines. Most existing deep learning libraries facilitate the development of neural nets by providing a mathematical framework that helps users implement their models more efficiently. This still represents a substantial investment of time and effort, however, when the intention is to compare a range of competing models quickly for a specific task. We present DEFIne, a fluent interface DSL for the specification, optimisation and evaluation of deep learning models. The fluent interface is implemented through method chaining. DEFIne is embedded in Python and is build on top of its most popular deep learning libraries, Keras and Theano. It extends these with common operations for data pre-processing and representation as well as visualisation of datasets and results. We test our framework on three benchmark tasks from different domains: heart disease diagnosis, hand-written digit recognition and weather forecast generation. Results in terms of accuracy, runtime and lines of code show that our DSL achieves equivalent accuracy and runtime to state-of-the-art models, while requiring only about 10 lines of code per application.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2017. In Proceedings of the 2nd International Workshop on Real World Domain Specific Languages (RWDSL), co-located
									with the International Symposium on Code Generation and Optimisation (CGO’17). Austin, Texas. In: ACM Digital Library,
									International Conference Proceedings Series (ICPS).</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Artificial Epigenetic Networks: Automatic Decomposition of Dynamical Control Tasks Using Topological Self-Modification.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="alex.html">Turner, A.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>Caves, L.</li>
								<li>
									<i class="fa fa-user"></i>Stepney, S.</li>
								<li>
									<i class="fa fa-user"></i>Tyrrell, A.</li>
								<li>
									<i class="fa fa-user"></i>Lones, M.</li>
								<li>
									<a href="publications/Turner_et_al_ArtificialEpigeneticNetworks_TNNLS.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										This paper describes the artificial epigenetic network, a recurrent connectionist architecture that is able to dynamically modify its topology in order to automatically decompose and solve dynamical problems. The approach is motivated by the behavior of gene regulatory networks, particularly the epigenetic process of chromatin remodeling that leads to topological change and which underlies the differentiation of cells within complex biological organisms. We expected this approach to be useful in situations where there is a need to switch between different dynamical behaviors, and do so in a sensitive and robust manner in the absence of a priori information about problem structure. This hypothesis was tested using a series of dynamical control tasks, each requiring solutions that could express different dynamical behaviors at different stages within the task. In each case, the addition of topological self-modification was shown to improve the performance and robustness of controllers. We believe this is due to the ability of topological changes to stabilize attractors, promoting stability within a dynamical regime while allowing rapid switching between different regimes. Post hoc analysis of the controllers also demonstrated how the partitioning of the networks could provide new insights into problem structure.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2016. IEEE Transactions on neural networks and learning systems.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="evolutionary_algorithms.html#publications">
									<i class="fa fa-tag"></i>Evolutionary Algorithms</a>
								<a href="bio_health.html#publications">
									<i class="fa fa-tag"></i>Biology and Health</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->


					
					
					
				</main>

		</div>
		<!-- /Container -->

	</div>
	<!-- /Portfolio -->		



	<!-- Footer -->
	<footer id="footer" class="sm-padding bg-dark">

		<!-- Container -->
		<div class="container">

			<!-- Row -->
			<div class="row">

				<div class="col-md-12">

					<!-- footer logo -->
					<div class="footer-logo">
						<a href="index.html"><img src="img/logo-alt.png" alt="logo"></a>
					</div>
					<!-- /footer logo -->

					<!-- footer copyright -->
					<div class="footer-copyright">
						<p>Copyright © 2017. All Rights Reserved. Designed by <a href="https://colorlib.com" target="_blank">Colorlib</a></p>
					</div>
					<!-- /footer copyright -->

				</div>

			</div>
			<!-- /Row -->

		</div>
		<!-- /Container -->

	</footer>
	<!-- /Footer -->

	<!-- Back to top -->
	<div id="back-to-top"></div>
	<!-- /Back to top -->

	<!-- Preloader -->
	<div id="preloader">
		<div class="preloader">
			<span></span>
			<span></span>
			<span></span>
			<span></span>
		</div>
	</div>
	<!-- /Preloader -->

	<!-- jQuery Plugins -->
	<script type="text/javascript" src="js/jquery.min.js"></script>
	<script type="text/javascript" src="js/bootstrap.min.js"></script>
	<script type="text/javascript" src="js/owl.carousel.min.js"></script>
	<script type="text/javascript" src="js/jquery.magnific-popup.js"></script>
	<script type="text/javascript" src="js/main.js"></script>

</body>

</html>
