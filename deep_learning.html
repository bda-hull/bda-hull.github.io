<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

	<title>Big Data Analytics Research Group</title>

	<!-- Google font -->
	<link href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CVarela+Round" rel="stylesheet">

	<!-- Bootstrap -->
	<link type="text/css" rel="stylesheet" href="css/bootstrap.min.css" />

	<!-- Owl Carousel -->
	<link type="text/css" rel="stylesheet" href="css/owl.carousel.css" />
	<link type="text/css" rel="stylesheet" href="css/owl.theme.default.css" />

	<!-- Magnific Popup -->
	<link type="text/css" rel="stylesheet" href="css/magnific-popup.css" />

	<!-- Font Awesome Icon -->
	<link rel="stylesheet" href="css/font-awesome.min.css">

	<!-- Custom stlylesheet -->
	<link type="text/css" rel="stylesheet" href="css/style.css" />

	<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
		<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
		<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
</head>

<body>

	<!-- Header -->
	<header>

		<!-- Nav -->
		<nav id="nav" class="navbar">
			<div class="container">

				<div class="navbar-header">
					<!-- Logo -->
					<div class="navbar-brand">
						<a href="index.html">
							<img class="logo" src="img/logo.png" alt="logo">
						</a>
					</div>
					<!-- /Logo -->

					<!-- Collapse nav button -->
					<div class="nav-collapse">
						<span></span>
					</div>
					<!-- /Collapse nav button -->
				</div>

			<!--  Main navigation  -->
				<ul class="main-nav nav navbar-nav navbar-right">
					<li><a href="index.html#home">Home</a></li>
					<li><a href="index.html#research">Research</a>
					<li><a href="publications.html">Publications</a></li>
					<li><a href="index.html#team">Team</a></li>
					<li><a href="index.html#news">News, Projects + Activities</a></li>
					<li><a href="index.html#contact">Contact</a></li>
				</ul>
				<!-- /Main navigation -->

			</div>
		</nav>
		<!-- /Nav -->

		<!-- header wrapper -->
		<div class="header-wrapper sm-padding bg-grey">
			<div class="container">
				<h2>DEEP LEARNING</h2>
				<ul class="breadcrumb">
					<li class="breadcrumb-item"><a href="index.html">Home</a></li>
					<li class="breadcrumb-item"><a href="index.html#research">Research</a></li>
				</ul>
			</div>
		</div>
		<!-- /header wrapper -->

	</header>
	<!-- /Header -->

	<!-- Blog -->
	<div id="blog" class="section md-padding">

		<!-- Container -->
		<div class="container">
			<!-- Row -->
			<div class="row">

				<!-- Main -->
				<main id="main" class="col-md-9">
					<div class="blog">
						<div class="blog-img">
<!--					<img class="img-responsive" src="./img/rnnc.jpg" alt="" width="800">-->							
						</div>
						<div class="blog-content">

							<h3><br/> Our Deep Learning Research</h3>
							
							<p>We apply machine learning in all our research projects, most recently deep learning in particular due to its suitability for feature learning from large datasets and its ability to identify patterns in large and noisy datasets. </p>								
														<img class="img-responsive" src="./img/rnnc.jpg" alt="" width="350" align="right">																					
							<p>Most of our work on natural language processing and other time-series applications uses LSTM/GRU models (e.g. for <a href="publications/wisdom2018schoene.pdf">sentiment analysis</a>, <a href="publications/ieee_ci_final_draft.pdf">natural language generation</a>) while other projects in renewable energy and healthcare have used mainly CNNs.</p>							
							<p>Current projects include a PhD scholarship by <a href="http://greenporthull.co.uk/">Greenport Growth</a>  for deep learning for offshore wind energy in collaboration with <a href="https://aurawindenergy.com/">Project Aura</a> and 5 University PhD scholarships for work on or related to deep learning. </p><p>We run and host the annual Deep Learning Winter School (funded by the Ferens Education Trust) and offer a range of student projects in this area too. </br></br></p>										
							
<h5>Language Learning and Domain Transfer</h5>							
							<p>Research beyond applications of deep learning to specific domains looks mainly into domain transfer of deep learning models. For example, we have developed a technique that allows <a href="nlp.html">natural language generation systems</a> to produce outputs for completely new and unseen domains by drawing on a set of common abstract linguistic inputs identified from other domains and achieving up to <a href="publications/ieee_ci_final_draft.pdf">100% of source domain performance</a>. This is useful as it reduces the need for large amounts of clean and high-quality data for each new domain, thereby reducing development time, costs and human effort and making a significant step towards automatic induction of language generation systems. </br></br></p>	
							
<h5>Fast Protoyping</h5>							
										
							<img class="img-responsive" src="./img/pca.jpg" alt="" width="250" align="right">														
							<p>We have also investigated shortcuts in developing deep learning models for new domains per se, including domain specific programming languages for fast prototyping based on an underspecific input representation that is automatically optimised for a new learning problem. Other research includes domain transfer for hyperparameter optimisation. Here, we found that development time for deep learning models can be reduced by up to 70% by reusing parameters that are known to be optimal in a domain that is similar to a current target domain across a number of objective characteristics.</p>																	
						</div>

																	
	
	<!-- Portfolio -->
	<div id="projects" class="section md-padding">

		<!-- Container -->
		<div class="container">

			<!-- Row -->
			<div class="row">						
					
				</main>
				<!-- /Main -->


				<!-- Aside -->
				<aside id="aside" class="col-md-3">

					<!-- Category -->
					<div class="widget">
						<h3 class="title">Categories</h3>
						<div class="widget-category">
							<a href="deep_learning.html#main">Deep Learning</a>
							<a href="evolutionary_algorithms.html#main">Evolutionary Algorithms</a>
							<a href="interactive_systems.html#main">Interactive Systems</a>
							<a href="nlp.html#main">Natural Language Processing</a>
							<a href="bio_health.html#main">Biology and Health</a>
							<a href="natural.html#main">Data and Natural World</a>							
						</div>
					</div>
					<!-- /Category -->

					<!-- Posts sidebar -->
					<div class="widget">
						<h3 class="title">News Stuff</h3>
						
						<!-- single post -->
						<div class="widget-post">
							<a href="https://www.eventbrite.com/e/gpu-programming-for-deep-learning-tickets-53947006950">
								<img src="./img/rnnc2-post.jpg" alt=""> GPU programming for deep learning - 24/25 Jan.
							</a>
							<ul class="blog-meta">
								<li>20 Dec 2018</li>
							</ul>
						</div>
						<!-- /single post -->						
						
					<!-- single post -->
						<div class="widget-post">
							<a href="news-xmas-2018.html">
								<img src="./img/xmas-2018-post.jpg" alt=""> Christmas Poster Workshop 2018
							</a>
							<ul class="blog-meta">
								<li>7 Dec 2018</li>
							</ul>
						</div>
						<!-- /single post -->							
						
						<!-- single post -->
						<div class="widget-post">
							<a href="news-catapult-joyjit-2018.html">
								<img src="./img/catapult-post.jpg" alt=""> OREC Catapult in Glasgow
							</a>
							<ul class="blog-meta">
								<li>22 Nov 2018</li>
							</ul>
						</div>
						<!-- /single post -->						
						
						<!-- single post -->
						<div class="widget-post">
							<a href="./img/xmas-2018.jpg">
								<img src="./img/xmas-post.jpg" alt=""> Christmas poster workshop and lunch
							</a>
							<ul class="blog-meta">
								<li>27 Oct 2018</li>
							</ul>
						</div>
						<!-- /single post -->

						<!-- single post -->
						<div class="widget-post">
							<a href="https://www.eventbrite.co.uk/e/deep-learning-winter-school-2019-tickets-51874180069">
								<img src="./img/rnnc2-post.jpg" alt=""> 2nd Deep Learning Winter School - 22/23 Jan.
							</a>
							<ul class="blog-meta">
								<li>26 Oct 2018</li>
							</ul>
						</div>
						<!-- /single post -->

						<!-- single post -->
						<div class="widget-post">
							<a href="news-aura-joyjit-2018.html">
								<img src="./img/aura-joyjit-post.jpg" alt=""> Aura wind energy PhDs - Class 2018.
							</a>
							<ul class="blog-meta">
								<li>25 Oct 2018</li>
							</ul>
						</div>
						<!-- /single post -->

						<!-- single post -->
						<div class="widget-post">
							<a href="https://jobs.hull.ac.uk/Vacancy.aspx?ref=FS0392">
								<img src="./img/epi-post.jpg" alt=""> PDRA in Transparent AI - closes 15 Nov.
							</a>
							<ul class="blog-meta">
								<li>22 Oct 2018</li>
							</ul>
						</div>
						<!-- /single post -->

						<a href="index.html#news"> --- ALL NEWS --- </a>																			

					</div>
					<!-- /Posts sidebar -->

				</aside>
				<!-- /Aside -->

			</div>
			<!-- /Row -->

		</div>
		<!-- /Container -->

	</div>
	<!-- /Blog -->
	
	
	
	
	<!-- Portfolio -->
	<div id="publications" class="section md-padding bg-grey">

		<!-- Container -->
		<div class="container">

			<!-- Row -->
			<div class="row">

				<!-- Section header -->
				<div class="section-header text-center">
					<h2 class="title">Deep Learning Publications</h2>
				</div>

			<main class="section ">
					<div class="blog">


						<!-- publication entry -->

						<div class="blog-content">
							<h3>Unsupervised suicide note classification.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="annika.html">Schoene, A.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
									<li>
									<a href="publications/wisdom2018schoene.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>	
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										With the greater availability of linguistic data from public social media platforms and the advancements of natural language processing, a number of opportunities have arisen for researchers to analyse this type of data. Research efforts have mostly focused on detecting the polarity of textual data, evaluating whether there is positive, negative or sometimes neutral content. Especially the use of neural networks has recently yielded significant results in polarity detection experiments. In this paper we present a more fine-grained approach to detecting sentiment in textual data, particularly analysing a corpus of suicide notes, depressive notes and love notes. We achieve a classification accuracy of 71.76% when classifying based on text and sentiment features, and an accuracy of 69.41% when using the words present in the notes alone. We discover that while emotions in all three datasets overlap, each of them has a unique ‘emotion profile’ which allows us to draw conclusions about the potential mental state that is reflects. Using the emotion sequences only, we achieve an accuracy of 75.29%. The results from unannotated data, while worse than the other models, nevertheless represent an encouraging step towards being able to flag potentially harmful social media posts online and in real time. We provide a high-level corpus analysis of the data sets in order to demonstrate the grammatical and emotional differences.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2018. In Proceedings of the 7th KDD Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM), co-located
									with the Knowledge Discovery and Data Mining (KDD), London, UK.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Domain Transfer for Deep Natural Language Generation from Abstract Meaning Representations.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<a href="publications/ieee_ci_final_draft.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										Stochastic natural language generation systems that are trained from labelled datasets are often domain-specific in their annotation and in their mapping from semantic input representations to lexical-syntactic outputs. As a result, learnt models fail to generalize across domains, heavily restricting their usability beyond single applications. In this article, we focus on the problem of domain adaptation for natural language generation. We show how linguistic knowledge from a source domain, for which labelled data is available, can be adapted to a target domain by reusing training data across domains. As a key to this, we propose to employ abstract meaning representations as a common semantic representation across domains. We model natural language generation as a long short- term memory recurrent neural network encoder-decoder, in which one recurrent neural network learns a latent representation of a semantic input, and a second recurrent neural network learns to decode it to a sequence of words. We show that the learnt representations can be transferred across domains and can be leveraged effectively to improve training on new unseen domains. Experiments in three different domains and with six datasets demonstrate that the lexical-syntactic constructions learnt in one domain can be transferred to new domains and achieve up to 75-100% of the performance of in-domain training. This is based on objective metrics such as BLEU and semantic error rate and a subjective human rating study. Training a policy from prior knowledge from a different domain is consistently better than pure in-domain training by up to 10%.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2017. IEEE Computational Intelligence Magazine: Special Issue on Natural Language Generation with Computational Intelligence.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
								<a href="interactive_systems.html#publications">
									<i class="fa fa-tag"></i>Interactive Systems</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Transparency Of Execution Using Epigenetic Networks.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="alex.html">Turner, A.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<a href="https://www.mitpressjournals.org/doi/abs/10.1162/ecal_a_0068_14"><i class="fa fa-external-link"></i></a>Link to article</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										This paper describes how the recurrent connectionist architecture epiNet, which is capable of dynamically modifying its topology, is able to provide a form of transparent execution. EpiNet, which is inspired by eukaryotic gene regulation in nature, is able to break its own architecture down into sets of smaller interacting networks. This allows for autonomous complex task decomposition, and by analysing these smaller interacting networks, it is possible to provide a real world understanding of why specific decisions have been made. We expect this work to be useful in fields where the risk of improper decision making is high, such as medical simulations, diagnostics and financial modelling. To test this hypothesis we apply epiNet to two data sets within UCI’s machine learning repository, each of which requires a specific set of behaviours to solve. We then perform analysis on the overall functionality of epiNet in order to deduce the underlying rules behind its functionality and in turn provide transparency of execution.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2017. In Proceedings of the European Conference on Artificial Life (ECAL), Lyon, France.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="evolutionary_algorithms.html#publications">
									<i class="fa fa-tag"></i>Evolutionary Algorithms</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Deep text generation - Using hierarchical decomposition to mitigate the effect of rare data points.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>
									<a href="alex.html">Turner, A.</a>
								</li>
								<li>
									<a href="publications/ldk2017.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										Deep learning has recently been adopted for the task of natural language generation (NLG) and shown remarkable results. However, learning can go awry when the input dataset is too small or not well balanced with regards to the examples it contains for various input sequences. This is relevant to naturally occurring datasets such as many that were not prepared for the task of natural language processing but scraped off the web and originally prepared for a different purpose. As a mitigation to the problem of unbalanced training data, we therefore propose to decompose a large natural language dataset into several subsets that “talk about” the same thing. We show that the decomposition helps to focus each learner’s attention during training. Results from a proof-of-concept study show 73% times faster learning over a flat model and better results.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2017. In Proceedings of Language, Data and Knowledge (LDK), Galway, Ireland. Proceedings in: Springer Lecture Notes
									in Computer Science (LNCS).</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
								<a href="nlp.html#publications">
									<i class="fa fa-tag"></i>Natural Language Processing</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						<!-- publication entry -->

						<div class="blog-content">
							<h3>DEFIne: A Fluent Interface DSL for Deep Learning Applications.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="nina.html">Dethlefs, N.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>Hawick, K.</li>
								<li>
									<a href="publications/rwdsl2017-final.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										Recent years have seen a surge of interest in deep learning models that outperform other machine learning algorithms on benchmarks across many disciplines. Most existing deep learning libraries facilitate the development of neural nets by providing a mathematical framework that helps users implement their models more efficiently. This still represents a substantial investment of time and effort, however, when the intention is to compare a range of competing models quickly for a specific task. We present DEFIne, a fluent interface DSL for the specification, optimisation and evaluation of deep learning models. The fluent interface is implemented through method chaining. DEFIne is embedded in Python and is build on top of its most popular deep learning libraries, Keras and Theano. It extends these with common operations for data pre-processing and representation as well as visualisation of datasets and results. We test our framework on three benchmark tasks from different domains: heart disease diagnosis, hand-written digit recognition and weather forecast generation. Results in terms of accuracy, runtime and lines of code show that our DSL achieves equivalent accuracy and runtime to state-of-the-art models, while requiring only about 10 lines of code per application.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2017. In Proceedings of the 2nd International Workshop on Real World Domain Specific Languages (RWDSL), co-located
									with the International Symposium on Code Generation and Optimisation (CGO’17). Austin, Texas. In: ACM Digital Library,
									International Conference Proceedings Series (ICPS).</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->

						

						<!-- publication entry -->

						<div class="blog-content">
							<h3>Artificial Epigenetic Networks: Automatic Decomposition of Dynamical Control Tasks Using Topological Self-Modification.</h3>
							<ul class="blog-meta">
								<li>
									<i class="fa fa-user"></i>
									<a href="alex.html">Turner, A.</a>
								</li>
								<li>
									<i class="fa fa-user"></i>Caves, L.</li>
								<li>
									<i class="fa fa-user"></i>Stepney, S.</li>
								<li>
									<i class="fa fa-user"></i>Tyrrell, A.</li>
								<li>
									<i class="fa fa-user"></i>Lones, M.</li>
								<li>
									<a href="publications/Turner_et_al_ArtificialEpigeneticNetworks_TNNLS.pdf"><i class="fa fa-cloud-download"></i></a>PDF</li>
							</ul>

							<ul class="blog-meta">
								<abstract style="display:inline" ;>
									<details open style="display:inline;">
										<summary>
											<span style="color:#6195FF">Hide/Show Full Abstract</span>
										</summary>
										This paper describes the artificial epigenetic network, a recurrent connectionist architecture that is able to dynamically modify its topology in order to automatically decompose and solve dynamical problems. The approach is motivated by the behavior of gene regulatory networks, particularly the epigenetic process of chromatin remodeling that leads to topological change and which underlies the differentiation of cells within complex biological organisms. We expected this approach to be useful in situations where there is a need to switch between different dynamical behaviors, and do so in a sensitive and robust manner in the absence of a priori information about problem structure. This hypothesis was tested using a series of dynamical control tasks, each requiring solutions that could express different dynamical behaviors at different stages within the task. In each case, the addition of topological self-modification was shown to improve the performance and robustness of controllers. We believe this is due to the ability of topological changes to stabilize attractors, promoting stability within a dynamical regime while allowing rapid switching between different regimes. Post hoc analysis of the controllers also demonstrated how the partitioning of the networks could provide new insights into problem structure.
									</details>
								</abstract>
							</ul>
							<ul class="blog-meta">
								<li>2016. IEEE Transactions on neural networks and learning systems.</li>
							</ul>
							<!-- blog tags -->
							<div class="blog-tags">
								<h5>Tags :</h5>
								<a href="evolutionary_algorithms.html#publications">
									<i class="fa fa-tag"></i>Evolutionary Algorithms</a>
								<a href="bio_health.html#publications">
									<i class="fa fa-tag"></i>Biology and Health</a>
								<a href="deep_learning.html#publications">
									<i class="fa fa-tag"></i>Deep Learning</a>
							</div>
						</div>
						<!-- blog tags -->

						<!-- publication entry -->


					
					
					
				</main>

		</div>
		<!-- /Container -->

	</div>
	<!-- /Portfolio -->		



	<!-- Footer -->
	<footer id="footer" class="sm-padding bg-dark">

		<!-- Container -->
		<div class="container">

			<!-- Row -->
			<div class="row">

				<div class="col-md-12">

					<!-- footer logo -->
					<div class="footer-logo">
						<a href="index.html"><img src="img/logo-alt.png" alt="logo"></a>
					</div>
					<!-- /footer logo -->

					<!-- footer copyright -->
					<div class="footer-copyright">
						<p>Copyright © 2017. All Rights Reserved. Designed by <a href="https://colorlib.com" target="_blank">Colorlib</a></p>
					</div>
					<!-- /footer copyright -->

				</div>

			</div>
			<!-- /Row -->

		</div>
		<!-- /Container -->

	</footer>
	<!-- /Footer -->

	<!-- Back to top -->
	<div id="back-to-top"></div>
	<!-- /Back to top -->

	<!-- Preloader -->
	<div id="preloader">
		<div class="preloader">
			<span></span>
			<span></span>
			<span></span>
			<span></span>
		</div>
	</div>
	<!-- /Preloader -->

	<!-- jQuery Plugins -->
	<script type="text/javascript" src="js/jquery.min.js"></script>
	<script type="text/javascript" src="js/bootstrap.min.js"></script>
	<script type="text/javascript" src="js/owl.carousel.min.js"></script>
	<script type="text/javascript" src="js/jquery.magnific-popup.js"></script>
	<script type="text/javascript" src="js/main.js"></script>

</body>

</html>
